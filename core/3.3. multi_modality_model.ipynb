{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207c8d80",
   "metadata": {},
   "source": [
    "# preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ff7475",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cd8b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment imports\n",
    "import sys, os, shutil, glob, random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d6dadf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math, timm, time\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch import block\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from   torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from   torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from   torch.utils.data.dataloader import default_collate\n",
    "from   torch.optim.optimizer import Optimizer\n",
    "from   torch.nn import TransformerEncoderLayer\n",
    "from   torch.nn import Parameter\n",
    "\n",
    "# Torchvision import block\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# einpos import block\n",
    "import einops\n",
    "from   einops import rearrange, repeat\n",
    "from   einops.layers.torch import Rearrange\n",
    "\n",
    "from   tqdm.notebook import tqdm\n",
    "from   scipy import stats\n",
    "\n",
    "from   typing import Optional, Tuple\n",
    "import copy\n",
    "from   IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "b433f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a805c198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current packages version:\n",
      "* torch: 2.2.1;\n",
      "* timm: 0.6.12;\n",
      "* torchvision: 0.17.1\n"
     ]
    }
   ],
   "source": [
    "print(f'current packages version:\\n* torch: {torch.__version__};' \\\n",
    "      + f'\\n* timm: {timm.__version__};\\n* torchvision: {torchvision.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9264a4a",
   "metadata": {},
   "source": [
    "## variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "762f8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR, LOCAL_DIR, FE_MODELS_DIR = './data', './', './models/feature_extractors'\n",
    "\n",
    "TRAIN_MP4_DIR, TRAIN_WAV_DIR = f'{DATA_DIR}/train/mp4', f'{DATA_DIR}/train/wav'\n",
    "VALID_MP4_DIR, VALID_WAV_DIR = f'{DATA_DIR}/val/mp4', f'{DATA_DIR}/val/wav'\n",
    "\n",
    "TRAIN_MP4_FEATURES_DIR, TRAIN_WAV_FEATURES_DIR = f'{DATA_DIR}/train/mp4_features', f'{DATA_DIR}/train/wav_features'\n",
    "VALID_MP4_FEATURES_DIR, VALID_WAV_FEATURES_DIR = f'{DATA_DIR}/val/mp4_features', f'{DATA_DIR}/val/wav_features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94e76bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABAW5_MODELS_CHECKPOINTS = './models/abaw_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d87e0378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current connected device is cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Current connected device is {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da65355",
   "metadata": {},
   "source": [
    "## classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e15bf7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_prob(\n",
    "    features:np.ndarray=None, classifier_weights:np.ndarray=None, classifier_bias:np.ndarray=None, \n",
    "    logits:bool=True\n",
    ") -> np.ndarray:\n",
    "    '''\n",
    "    Function for the getting probabilities of the classes of the feature_extraction model.\n",
    "\n",
    "            Parameters:\n",
    "                features (np.ndarray): Current extracted features;\n",
    "                classifier_weights (np.ndarray): Classifier weights;\n",
    "                classifier_bias(np.ndarray): Classifier bias;\n",
    "                logits (bool): Get the logits or not.\n",
    "    '''\n",
    "    xs = np.dot(features, np.transpose(classifier_weights)) + classifier_bias\n",
    "\n",
    "    if logits:\n",
    "        return xs\n",
    "    else:\n",
    "        e_x = np.exp(xs - np.max(xs, axis=1)[:,np.newaxis])\n",
    "        return e_x / e_x.sum(axis=1)[:, None]\n",
    "\n",
    "def stack_npy_files(directory: str=None):\n",
    "    \"\"\"\n",
    "    Open all .npy files in the given directory and stack them into one big array.\n",
    "    \n",
    "            Parameters:\n",
    "                directory (str): The directory containing the .npy files.\n",
    "    \"\"\"\n",
    "    npy_files = [file for file in os.listdir(directory) if file.endswith('.npy')]\n",
    "    if not npy_files:\n",
    "        print(\"No .npy files found in the directory.\")\n",
    "        return None\n",
    "    \n",
    "    stacked_array = np.concatenate([np.load(os.path.join(directory, file)) for file in npy_files], axis=0)\n",
    "    return stacked_array\n",
    "\n",
    "class MultiModalityDataset(Dataset):\n",
    "    \"\"\"Dataset class for the multi-modality\"\"\"\n",
    "    def __init__(\n",
    "        self, data_info_path: str=None, split: str='Train', visual_feature_extractor_head: timm.models.EfficientNet=None,\n",
    "        HSEmotion_feature_dir_path: str=None, HSEmotion_scores: bool=True, HSEmotion_seq_length: int=64,\n",
    "        emotion2vec_feature_dir_path: str=None, emotion2vec_seq_length: int=64, use_rolling_mean: bool=True,\n",
    "        rolling_mean_step: int=8, OpenFace_feature_dir_path: str=None, OpenFace_seq_length: int=64\n",
    "    ):\n",
    "        super(MultiModalityDataset, self).__init__()\n",
    "        self.data_info, self.split = pd.read_csv(data_info_path), split\n",
    "        self.data_info = self.data_info[self.data_info['Split'] == self.split]\n",
    "        self.target_labels = ['Adoration', 'Amusement', 'Anxiety', 'Disgust','Empathic-Pain', 'Fear', 'Surprise']\n",
    "        \n",
    "        # visual-block\n",
    "        self.visual_feature_extractor_head, self.HSEmotion_feature_dir_path = visual_feature_extractor_head, \\\n",
    "            HSEmotion_feature_dir_path\n",
    "        self.HSEmotion_scores, self.HSEmotion_seq_length = HSEmotion_scores, HSEmotion_seq_length\n",
    "        \n",
    "        if self.HSEmotion_scores:\n",
    "            self.HSEmotion_weights = self.visual_feature_extractor_head.classifier.weight.cpu().data.numpy()\n",
    "            self.HSEmotion_bias = self.visual_feature_extractor_head.classifier.bias.cpu().data.numpy()\n",
    "            \n",
    "        # acoustic-block\n",
    "        self.emotion2vec_feature_dir_path, self.emotion2vec_seq_length = emotion2vec_feature_dir_path, \\\n",
    "            emotion2vec_seq_length\n",
    "        self.use_rolling_mean, self.rolling_mean_step = use_rolling_mean, rolling_mean_step\n",
    "        \n",
    "        # AU-block\n",
    "        self.OpenFace_feature_dir_path, self.OpenFace_seq_length = OpenFace_feature_dir_path, OpenFace_seq_length\n",
    "        \n",
    "        self.file_name_padding = 5\n",
    "        self.__init_inputs()\n",
    "        \n",
    "    def __init_inputs(self):\n",
    "        print(f'Configure dataset from directories')\n",
    "        self.inputs, self.labels = [], []\n",
    "        \n",
    "        for index, row_values in tqdm(self.data_info.iterrows(), total=self.data_info.shape[0]):\n",
    "            modality_inputs = {}\n",
    "            \n",
    "            current_ID = str(row_values['ID'])\n",
    "            current_file_ID = '0' * (self.file_name_padding - len(current_ID)) + current_ID\n",
    "            \n",
    "            HSEmotion_dir_path = f'{self.HSEmotion_feature_dir_path}/{current_file_ID}_batched_features'\n",
    "            emotion2vec_dir_path = f'{self.emotion2vec_feature_dir_path}/{current_file_ID}_features'\n",
    "            OpenFace_feature_path = f'{self.OpenFace_feature_dir_path}/{current_file_ID}.csv'\n",
    "            \n",
    "            if len(os.listdir(HSEmotion_dir_path)) < 1 or len(os.listdir(emotion2vec_dir_path)) < 1 or \\\n",
    "                not os.path.exists(OpenFace_feature_path): continue\n",
    "                \n",
    "            HSEmotion_features = stack_npy_files(directory=HSEmotion_dir_path)\n",
    "            emotion2vec_features = np.load(f'{emotion2vec_dir_path}/feature-extraction-model_features.npy')\n",
    "            OpenFace_features = pd.read_csv(OpenFace_feature_path).iloc[::5, 5:].iloc[:, 142:].values\n",
    "            \n",
    "            if self.HSEmotion_scores:\n",
    "                scores = []\n",
    "                for row in range(HSEmotion_features.shape[0]):\n",
    "                    row_visual_feature = HSEmotion_features[row, :]\n",
    "                    current_scores = get_prob(row_visual_feature, self.HSEmotion_weights, self.HSEmotion_bias)\n",
    "                    scores.append(current_scores)\n",
    "\n",
    "                scores = np.stack(scores, axis=0)\n",
    "                HSEmotion_features = np.concatenate((HSEmotion_features, scores), axis=1)\n",
    "            \n",
    "            # initialize rolling mean features\n",
    "            if self.use_rolling_mean:\n",
    "                dataframe = pd.DataFrame(emotion2vec_features)\n",
    "                emotion2vec_features = dataframe.groupby(dataframe.index // self.rolling_mean_step).mean().values\n",
    "                del dataframe\n",
    "            \n",
    "            HSEmotion_features_length = HSEmotion_features.shape[0]\n",
    "            emotion2vec_features_length = emotion2vec_features.shape[0]\n",
    "            OpenFace_features_length = OpenFace_features.shape[0]\n",
    "            \n",
    "            if self.HSEmotion_seq_length is not None:\n",
    "                if HSEmotion_features_length > self.HSEmotion_seq_length: \n",
    "                    HSEmotion_features = HSEmotion_features[:self.HSEmotion_seq_length]\n",
    "                else:\n",
    "                    HSEmotion_features = np.pad(\n",
    "                        HSEmotion_features, pad_width=((0, self.HSEmotion_seq_length - HSEmotion_features_length),(0,0)))\n",
    "               \n",
    "            if self.emotion2vec_seq_length is not None:\n",
    "                if emotion2vec_features_length > self.emotion2vec_seq_length: \n",
    "                    emotion2vec_features = emotion2vec_features[:self.emotion2vec_seq_length]\n",
    "                else:\n",
    "                    emotion2vec_features = np.pad(\n",
    "                        emotion2vec_features, pad_width=((0, self.emotion2vec_seq_length - emotion2vec_features_length),(0,0)))\n",
    "        \n",
    "            if self.OpenFace_seq_length is not None:\n",
    "                if OpenFace_features_length > self.OpenFace_seq_length: \n",
    "                    OpenFace_features = OpenFace_features[:self.OpenFace_seq_length]\n",
    "                else:\n",
    "                    OpenFace_features = np.pad(\n",
    "                        OpenFace_features, pad_width=((0, self.OpenFace_seq_length - OpenFace_features_length),(0,0)))\n",
    "            \n",
    "            modality_inputs['visual'] = HSEmotion_features\n",
    "            modality_inputs['acoustic'] = emotion2vec_features\n",
    "            modality_inputs['AUs'] = OpenFace_features\n",
    "            self.inputs.append(modality_inputs)\n",
    "            \n",
    "            current_labels = row_values[self.target_labels].values\n",
    "            self.labels.append(current_labels)\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index: int=None):\n",
    "        modality_input = self.inputs[index]\n",
    "        labels = self.labels[index]\n",
    "        \n",
    "        return modality_input, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc907b2",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106b9354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed:int=None) -> None:\n",
    "    '''\n",
    "    Function seed every random asprect.\n",
    "\n",
    "            Parameters:\n",
    "                seed (int): The seed number.\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def configure_feature_extraction_model_visual(\n",
    "    feature_extractor_model_path: str=None, device: torch.device=None, return_initial: bool=True\n",
    ") -> np.ndarray:\n",
    "    '''\n",
    "    Function configure feature-extraction model\n",
    "    \n",
    "            Parameters:\n",
    "                extraction_model_path (str): path to feature-extraction model;\n",
    "                device (torch.device): torch device (default=torch.cuda);\n",
    "                return_initial (bool): Return the initial model or not\n",
    "            Returns:\n",
    "                features-extraction model\n",
    "    '''\n",
    "    feature_extractor_model = torch.load(feature_extractor_model_path)\n",
    "    feature_extractor_model.classifier = torch.nn.Identity()\n",
    "    feature_extractor_model.to(device)\n",
    "    feature_extractor_model.eval()\n",
    "\n",
    "    if return_initial:\n",
    "        return feature_extractor_model, torch.load(feature_extractor_model_path)\n",
    "    else:\n",
    "        return feature_extractor_model\n",
    "    \n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def calc_pearsons(predictions:np.array=None, ground_truth:np.array=None):\n",
    "    '''\n",
    "    Function calculates Pearson's Correlation Coefficient.\n",
    "    \n",
    "            Parameters:\n",
    "                predictions (np.array): Model's forecasts;\n",
    "                ground_truth (np.array): The fact.\n",
    "    '''\n",
    "    # Replace NaN values with 0\n",
    "    predictions = np.nan_to_num(predictions, 1e-7)\n",
    "    ground_truth = np.nan_to_num(ground_truth, 1e-7)\n",
    "    \n",
    "    pcc = pearsonr(predictions, ground_truth)\n",
    "    return pcc[0]\n",
    "\n",
    "def mean_pearsons(predictions:np.array=None, ground_truth:np.array=None, n_classes:int=7):\n",
    "    '''\n",
    "    Function calculates mean PCC between predictions and fact.\n",
    "    \n",
    "            Parameters:\n",
    "                predictions (np.array): Model's forecasts;\n",
    "                ground_truth (np.array): The fact;\n",
    "                n_classes (int): number of classes.\n",
    "    '''\n",
    "    predictions, ground_truth = predictions.detach().cpu().numpy(), ground_truth.detach().cpu().numpy()\n",
    "    predictions = np.nan_to_num(predictions, 1e-7)\n",
    "    ground_truth = np.nan_to_num(ground_truth, 1e-7)\n",
    "    \n",
    "    class_wise_pcc = np.array([calc_pearsons(predictions[:, i], ground_truth[:, i]) for i in range(n_classes)])\n",
    "    mean_pcc = np.mean(class_wise_pcc)\n",
    "    \n",
    "    return mean_pcc, class_wise_pcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebc6775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_multi_modal(\n",
    "    model: nn.Module=None, train_dataloader: DataLoader=None, optimizer: torch.optim.Optimizer=None, \n",
    "    scheduler: torch.optim.lr_scheduler=None, device: torch.device=DEVICE, mode: str=None, criterion: nn.Module=None\n",
    "):\n",
    "    train_loss, train_apcc = 0, 0\n",
    "    model.train()\n",
    "    \n",
    "    for step, current_batch in enumerate(tqdm(train_dataloader, desc='train...')) :\n",
    "        features, labels = current_batch\n",
    "        stacked_labels = np.stack(labels, axis=0)\n",
    "        stakced_labels = np.array(stacked_labels, dtype=np.float)\n",
    "        \n",
    "        visual_features = torch.tensor(data=[feature['visual'] for feature in features], dtype=torch.float).to(device)\n",
    "        acoustic_features = torch.tensor(data=[feature['acoustic'] for feature in features], dtype=torch.float).to(device)\n",
    "        AUs_features = torch.tensor(data=[feature['AUs'] if isinstance(feature['AUs'], np.ndarray) else feature['AUs'].values for feature in features], dtype=torch.float).to(device)\n",
    "        labels = torch.tensor(data=stakced_labels, dtype=torch.float).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model_outputs = model(visual_features, acoustic_features, AUs_features)\n",
    "        batch_loss = criterion(labels, model_outputs)\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        train_loss += batch_loss.item()\n",
    "        \n",
    "        batch_mean_pcc, _ = mean_pearsons(model_outputs, labels)\n",
    "        train_apcc += batch_mean_pcc\n",
    "        \n",
    "        optimizer.step()\n",
    "            \n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "    train_apcc = train_apcc / len(train_dataloader)\n",
    "    \n",
    "    return train_loss, train_apcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "060e2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_epoch_multi_modal(\n",
    "    model: nn.Module=None, valid_dataloader: DataLoader=None, criterion: nn.Module=None, \n",
    "    scheduler: torch.optim.lr_scheduler=None, device: torch.device=DEVICE, mode: str=None,\n",
    "    model_checkpoint_dir: str=None, best_validation_score: float=None, model_suffix: str=None\n",
    "):\n",
    "    valid_loss, valid_apcc = 0, 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, current_batch in enumerate(tqdm(valid_dataloader, desc='valid...')) :\n",
    "            features, labels = current_batch\n",
    "            stacked_labels = np.stack(labels, axis=0)\n",
    "            stakced_labels = np.array(stacked_labels, dtype=np.float)\n",
    "\n",
    "            visual_features = torch.tensor(data=[feature['visual'] for feature in features], dtype=torch.float).to(device)\n",
    "            acoustic_features = torch.tensor(data=[feature['acoustic'] for feature in features], dtype=torch.float).to(device)\n",
    "            AUs_features = torch.tensor(data=[feature['AUs'] if isinstance(feature['AUs'], np.ndarray) else feature['AUs'].values for feature in features], dtype=torch.float).to(device)\n",
    "            labels = torch.tensor(data=stakced_labels, dtype=torch.float).to(device)\n",
    "            \n",
    "            model_outputs = model(visual_features, acoustic_features, AUs_features)\n",
    "            batch_loss = criterion(labels, model_outputs)\n",
    "            valid_loss += batch_loss.item()\n",
    "\n",
    "            batch_mean_pcc, _ = mean_pearsons(model_outputs, labels)\n",
    "            valid_apcc += batch_mean_pcc\n",
    "            \n",
    "    valid_loss = valid_loss / len(valid_dataloader)\n",
    "    valid_apcc = valid_apcc / len(valid_dataloader)\n",
    "    \n",
    "    if valid_apcc >= best_validation_score:\n",
    "        torch.save(model.state_dict(), f'{model_checkpoint_dir}/{model_suffix}_{valid_apcc:.4f}.pt')\n",
    "    \n",
    "    return valid_loss, valid_apcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbbd9ae",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe4a09",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2fa8c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(2002)\n",
    "_, feature_extractor_model = configure_feature_extraction_model_visual(\n",
    "    feature_extractor_model_path=f'{FE_MODELS_DIR}/efficientnet_affectnet.pt', device=DEVICE, return_initial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7130b3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configure dataset from directories\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fada9eaac474560b489cb850fd42fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15806 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configure dataset from directories\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20721708a97f44ae949c5278c339b410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset were configured in 20.0 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time_extraction = time.time()\n",
    "train_dataset = MultiModalityDataset(\n",
    "    data_info_path=f'{DATA_DIR}/preprocessed_data_info.csv', split='Train', \n",
    "    \n",
    "    # visual-block\n",
    "    visual_feature_extractor_head=feature_extractor_model, \n",
    "    HSEmotion_feature_dir_path=TRAIN_MP4_FEATURES_DIR + '-HSEmotion-aligned',\n",
    "    HSEmotion_scores=True, HSEmotion_seq_length=64,\n",
    "    \n",
    "    # acoustic-block\n",
    "    emotion2vec_feature_dir_path=TRAIN_WAV_FEATURES_DIR + '-feature-extraction-model',\n",
    "    emotion2vec_seq_length=64, use_rolling_mean=True, rolling_mean_step=10,\n",
    "    \n",
    "    # AU-block\n",
    "    OpenFace_feature_dir_path=f'{DATA_DIR}/train/mp4_features-OpenFace',\n",
    "    OpenFace_seq_length=64\n",
    ")\n",
    "\n",
    "valid_dataset = MultiModalityDataset(\n",
    "    data_info_path=f'{DATA_DIR}/preprocessed_data_info.csv', split='Val', \n",
    "    \n",
    "    # visual-block\n",
    "    visual_feature_extractor_head=feature_extractor_model, \n",
    "    HSEmotion_feature_dir_path=VALID_MP4_FEATURES_DIR + '-HSEmotion-aligned',\n",
    "    HSEmotion_scores=True, HSEmotion_seq_length=64,\n",
    "    \n",
    "    # acoustic-block\n",
    "    emotion2vec_feature_dir_path=VALID_WAV_FEATURES_DIR + '-feature-extraction-model',\n",
    "    emotion2vec_seq_length=64, use_rolling_mean=True, rolling_mean_step=10,\n",
    "    \n",
    "    # AU-block\n",
    "    OpenFace_feature_dir_path=f'{DATA_DIR}/val/mp4_features-OpenFace',\n",
    "    OpenFace_seq_length=64\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time_extraction\n",
    "print(f'Dataset were configured in {np.round((elapsed_time/60), 0)} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096bd6d1",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d2efb6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalCollator(object):\n",
    "    \"\"\"Class for appropriate collate_fn\"\"\"\n",
    "    def __init__(self, sequence_based: bool=False):\n",
    "        super().__init__()\n",
    "        self.sequence_based = sequence_based\n",
    "\n",
    "    def __call__(self, batch: torch.Tensor=None):\n",
    "        '''\n",
    "        Select a specfic number of images randomly for the time being.\n",
    "        \n",
    "                Parameters:\n",
    "                    batch (torch.Tensor): Current batch\n",
    "        '''\n",
    "        stacked_batch = np.stack(batch, axis=0)\n",
    "        features = stacked_batch[:, 0]\n",
    "        labels = stacked_batch[:, 1]\n",
    "        \n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "92c38f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = MultiModalCollator(sequence_based=False)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbf32ce",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "75f69a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.nn.init import xavier_uniform_, constant_\n",
    "\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size: int=None):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor=None):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_inputs: int=None, n_outputs: int=None, kernel_size: int=None, \n",
    "        stride: int=None, dilation: int=None, padding: int=None, dropout: float=0.2\n",
    "    ):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x: torch.Tensor=None):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs: int=None, num_channels: int=None, kernel_size: int=2, dropout: float=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_channels = num_channels\n",
    "        num_levels = len(num_channels)\n",
    "        self.out_channels = None\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [\n",
    "                TemporalBlock(\n",
    "                    in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                    padding=(kernel_size-1) * dilation_size, dropout=dropout\n",
    "                )]\n",
    "            \n",
    "            self.out_channels = out_channels\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor=None):\n",
    "        return self.network(x.transpose(1, 2)).transpose(1, 2) * math.sqrt(self.out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3a64a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LeakyReLU(),    \n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth=4, heads=6, dim_head=128, mlp_dim=126, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "    \n",
    "class MultiModalEncoder(nn.Module):\n",
    "    def __init__(self, layer, N, modal_num):\n",
    "        super(MultiModalEncoder, self).__init__()\n",
    "\n",
    "        self.modal_num = modal_num\n",
    "        self.layers = layer\n",
    "        self.norm = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.modal_num):\n",
    "            self.norm.append(LayerNorm(layer[0].size))\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        _x = torch.chunk(x, self.modal_num, dim=-1)\n",
    "        _x_list = []\n",
    "\n",
    "        for i in range(self.modal_num):\n",
    "            _x_list.append(self.norm[i](_x[i]))\n",
    "\n",
    "        x = torch.cat(_x_list, dim=-1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class MultiModalAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, modal_num, dropout=0.1):\n",
    "        super(MultiModalAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.modal_num = modal_num\n",
    "        self.mm_linears = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.modal_num):\n",
    "            linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "            self.mm_linears.append(linears)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = torch.chunk(query, self.modal_num, dim=-1)\n",
    "        key = torch.chunk(key, self.modal_num, dim=-1)\n",
    "        value = torch.chunk(value, self.modal_num, dim=-1)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        nbatches = query[0].size(0)\n",
    "        _query_list = []\n",
    "        _key_list = []\n",
    "        _value_list = []\n",
    "\n",
    "        for i in range(self.modal_num):\n",
    "            _query_list.append(self.mm_linears[i][0](\n",
    "                query[i]).view(nbatches, -1, self.h, self.d_k))\n",
    "\n",
    "            _key_list.append(self.mm_linears[i][1](\n",
    "                key[i]).view(nbatches, -1, self.h, self.d_k))\n",
    "\n",
    "            _value_list.append(self.mm_linears[i][2](\n",
    "                value[i]).view(nbatches, -1, self.h, self.d_k))\n",
    "\n",
    "        mm_query = torch.stack(_query_list, dim=-2)\n",
    "        mm_key = torch.stack(_key_list, dim=-2)\n",
    "        mm_value = torch.stack(_value_list, dim=-2)\n",
    "\n",
    "        x, _ = attention(mm_query, mm_key, mm_value,mask=mask, dropout=self.dropout)\n",
    "        x = x.transpose(-2, -3).contiguous().view(nbatches, - 1, self.modal_num, self.h * self.d_k)\n",
    "        _x = torch.chunk(x, self.modal_num, dim=-2)\n",
    "\n",
    "        _x_list = []\n",
    "\n",
    "        for i in range(self.modal_num):\n",
    "            _x_list.append(self.mm_linears[i][-1](_x[i].squeeze()))\n",
    "\n",
    "        x = torch.cat(_x_list, dim=-1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class MultiModalEncoderLayer(nn.Module):\n",
    "    def __init__(self, size, modal_num, mm_atten, mt_atten, feed_forward, dropout):\n",
    "        super(MultiModalEncoderLayer, self).__init__()\n",
    "\n",
    "        self.modal_num = modal_num\n",
    "        self.mm_atten = mm_atten\n",
    "        self.mt_atten = mt_atten\n",
    "        self.feed_forward = feed_forward\n",
    "\n",
    "        mm_sublayer = MultiModalSublayerConnection(size, modal_num, dropout)\n",
    "        mt_sublayer = nn.ModuleList()\n",
    "\n",
    "        for i in range(modal_num):\n",
    "            mt_sublayer.append(SublayerConnection(size, dropout))\n",
    "\n",
    "        ff_sublayer = nn.ModuleList()\n",
    "\n",
    "        for i in range(modal_num):\n",
    "            ff_sublayer.append(SublayerConnection(size, dropout))\n",
    "\n",
    "        self.sublayer = nn.ModuleList()\n",
    "        self.sublayer.append(mm_sublayer)\n",
    "        self.sublayer.append(mt_sublayer)\n",
    "        self.sublayer.append(ff_sublayer)\n",
    "\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.mm_atten(x, x, x))\n",
    "        _x = torch.chunk(x, self.modal_num, dim=-1)\n",
    "\n",
    "        _x_list = []\n",
    "\n",
    "        for i in range(self.modal_num):\n",
    "            feature = self.sublayer[1][i](_x[i], lambda x: self.mt_atten[i](x, x, x, mask=None))\n",
    "            feature = self.sublayer[2][i](feature, self.feed_forward[i])\n",
    "\n",
    "            _x_list.append(feature)\n",
    "\n",
    "        x = torch.cat(_x_list, dim=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ModalEncoder(nn.Module):\n",
    "    def __init__(self, dim=768, heads=6, depth=2, dim_head=64, dropout=0.3, mlp_dim=512, hidden_dim=512) -> None: \n",
    "        super().__init__() \n",
    "        self.att = Attention(dim = dim, heads = heads, dim_head = dim_head, dropout = dropout)\n",
    "        self.trans = Transformer(dim = dim, depth = depth, heads = heads, dim_head = dim_head, mlp_dim = mlp_dim, dropout = dropout)\n",
    "    def forward(self, x):\n",
    "        att = self.att(x)\n",
    "        trans = self.trans(x)\n",
    "        x = torch.cat((x, att, trans), 2)   \n",
    "        return x\n",
    "    \n",
    "from torch.autograd import Variable\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int=None, dropout: int=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        \n",
    "        v = torch.arange(0, d_model, 2).type(torch.float)\n",
    "        v = v * -(math.log(1000.0) / d_model)\n",
    "        div_term = torch.exp(v)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position.type(torch.float) * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.type(torch.float) * div_term)\n",
    "        pe = pe.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor=None):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e1025f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        nbatches = query.size(0)\n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) \n",
    "            for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        x, _ = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        x = x.transpose(1, 2).contiguous().view(\n",
    "            nbatches, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.linears[-1](x)\n",
    "    \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "    \n",
    "class MultiModalSublayerConnection(nn.Module):\n",
    "    def __init__(self, size, modal_num, dropout):\n",
    "        super(MultiModalSublayerConnection, self).__init__()\n",
    "\n",
    "        self.modal_num = modal_num\n",
    "        self.norm = nn.ModuleList()\n",
    "        \n",
    "        for i in range(self.modal_num):\n",
    "            self.norm.append(LayerNorm(size))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        residual = x\n",
    "        _x_list = []\n",
    "        _x = torch.chunk(x, self.modal_num, -1)\n",
    "\n",
    "        for i in range(self.modal_num):\n",
    "            _x_list.append(self.norm[i](_x[i]))\n",
    "\n",
    "        x = torch.cat(_x_list, dim=-1)\n",
    "        return self.dropout(sublayer(x)) + residual\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "\n",
    "        self.a_2 = nn.Parameter(torch.ones(features)).to(DEVICE)\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features)).to(DEVICE)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "    \n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    \n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cf4c40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectItem(nn.Module):\n",
    "    def __init__(self, item_index):\n",
    "        super(SelectItem, self).__init__()\n",
    "        self._name = 'selectitem'\n",
    "        self.item_index = item_index\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return inputs[self.item_index]\n",
    "    \n",
    "class SeqBatchNorm(nn.Module):\n",
    "    def __init__(self, num_features: int = 1290):\n",
    "        super(SeqBatchNorm, self).__init__()\n",
    "        self._name = 'reshapetensor'\n",
    "        self.shape_position = (0, 2, 1)\n",
    "        self.batchnorm = nn.BatchNorm1d(num_features=num_features)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        reshaped_inputs = inputs.permute(self.shape_position)\n",
    "        batched_normed = self.batchnorm(reshaped_inputs)\n",
    "        return batched_normed.permute(self.shape_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "16c15ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_3906(nn.Module): # 0.3906\n",
    "    def __init__(self, num_features: list=[1290, 768], d_model: int=128, tcn_levels: int=5, blocks_num: int=4):\n",
    "        super(Model_3906, self).__init__()\n",
    "        self.modal_num = len(num_features)\n",
    "        \n",
    "        self.input = nn.ModuleList()\n",
    "        for i in range(self.modal_num):\n",
    "            if i == 0:\n",
    "                self.input.append(\n",
    "                    nn.Sequential(\n",
    "                        nn.LayerNorm(num_features[i]),\n",
    "                        PositionalEncoding(d_model=num_features[i], dropout=0.3),\n",
    "                        ModalEncoder(dim=num_features[i], dropout=0.2),\n",
    "                        nn.Dropout(p=0.5),\n",
    "                        nn.Linear(in_features=num_features[i] * 3, out_features=d_model, bias=False)\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.input.append(\n",
    "                    nn.Sequential(\n",
    "                        nn.LayerNorm(num_features[i]),\n",
    "                        TemporalConvNet(num_inputs=num_features[i], num_channels=[d_model]),\n",
    "#                         PositionalEncoding(d_model=d_model, dropout=0.2),\n",
    "                        ModalEncoder(dim=d_model, dropout=0.4),\n",
    "                        nn.Linear(in_features=d_model * 3, out_features=d_model * 2, bias=False),\n",
    "                        nn.Dropout(p=0.1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(in_features=d_model * 2, out_features=d_model, bias=False)\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "        self.dropout_embed = nn.Dropout(p=0.4)\n",
    "        multimodal_encoder_layer = nn.ModuleList()\n",
    "        for i in range(8):\n",
    "            mm_attention = MultiModalAttention(\n",
    "                h=4, d_model=d_model, modal_num=self.modal_num, dropout=0.5)\n",
    "            \n",
    "            mt_attention, feed_forward = nn.ModuleList(), nn.ModuleList()\n",
    "            for j in range(self.modal_num):\n",
    "                mt_attention.append(MultiHeadedAttention(\n",
    "                    h=4, d_model=d_model, dropout=0.5))\n",
    "                feed_forward.append(PositionwiseFeedForward(\n",
    "                    d_model=d_model, d_ff=512, dropout=0.4))\n",
    "            \n",
    "            multimodal_encoder_layer.append(MultiModalEncoderLayer(\n",
    "                size=d_model, modal_num=self.modal_num, mm_atten=mm_attention, mt_atten=mt_attention,\n",
    "                feed_forward=feed_forward, dropout=0.5\n",
    "            ))\n",
    "        self.encoder = MultiModalEncoder(layer=multimodal_encoder_layer, N=8, modal_num=self.modal_num)\n",
    "        \n",
    "        self.regress = nn.Sequential(\n",
    "            nn.Linear(in_features=512, out_features=512 // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512 // 2),\n",
    "            nn.Linear(in_features=512 // 2, out_features=7)\n",
    "        )\n",
    "        \n",
    "        self.final_activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, v_input: torch.Tensor=None, a_input: torch.Tensor=None, au_input: torch.Tensor=None):\n",
    "        batch_size, _, _ = v_input.shape\n",
    "        va_input = [v_input, a_input]\n",
    "        \n",
    "        _x_list = []\n",
    "        for i in range(self.modal_num):\n",
    "            if i == 0:\n",
    "                _x_list.append(self.input[i](va_input[i]))\n",
    "            else:\n",
    "                _x_list.append(self.input[i](va_input[i]))\n",
    "            \n",
    "        x = torch.cat(_x_list, dim=-1)\n",
    "        x = self.dropout_embed(x)\n",
    "        \n",
    "        out = self.encoder(x, mask=None)\n",
    "        outs = self.final_activation(self.regress(torch.cat((out, x), dim=-1)))\n",
    "        return outs.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "435e98a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_3975(nn.Module): # 0.3975\n",
    "    def __init__(\n",
    "        self, modalities_features: dict={'visual': 1290, 'acoustic': 768, 'AUs': 34}, d_model: int=128\n",
    "    ):\n",
    "        super(Model_3975, self).__init__()\n",
    "        self.modal_num = 2\n",
    "        \n",
    "        self.input = nn.ModuleDict()\n",
    "        for modality_type, num_features in modalities_features.items():\n",
    "            if modality_type == 'visual':\n",
    "                self.input[modality_type] = nn.Sequential(\n",
    "                        nn.LayerNorm(num_features + modalities_features['AUs']),\n",
    "                        PositionalEncoding(d_model=num_features + modalities_features['AUs'], dropout=0.5),\n",
    "                        ModalEncoder(dim=num_features + modalities_features['AUs'], dropout=0.5),\n",
    "                        nn.Linear(in_features=(num_features + modalities_features['AUs']) * 3, out_features=d_model)\n",
    "                    )\n",
    "                \n",
    "            elif modality_type == 'acoustic':\n",
    "                self.input[modality_type] = nn.Sequential(\n",
    "                        nn.LayerNorm(num_features),\n",
    "                        TemporalConvNet(num_inputs=num_features, num_channels=[d_model]),\n",
    "#                         PositionalEncoding(d_model=d_model, dropout=0.2),\n",
    "                        ModalEncoder(dim=d_model, dropout=0.4),\n",
    "                        nn.Linear(in_features=d_model * 3, out_features=d_model * 2),\n",
    "                        nn.Dropout(p=0.1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(in_features=d_model * 2, out_features=d_model)\n",
    "                    )\n",
    "                \n",
    "        self.dropout_embed = nn.Dropout(p=0.4)\n",
    "        multimodal_encoder_layer = nn.ModuleList()\n",
    "        for i in range(8):\n",
    "            mm_attention = MultiModalAttention(\n",
    "                h=4, d_model=d_model, modal_num=self.modal_num, dropout=0.5)\n",
    "            \n",
    "            mt_attention, feed_forward = nn.ModuleList(), nn.ModuleList()\n",
    "            for j in range(self.modal_num):\n",
    "                mt_attention.append(MultiHeadedAttention(\n",
    "                    h=4, d_model=d_model, dropout=0.5))\n",
    "                feed_forward.append(PositionwiseFeedForward(\n",
    "                    d_model=d_model, d_ff=512, dropout=0.4))\n",
    "            \n",
    "            multimodal_encoder_layer.append(MultiModalEncoderLayer(\n",
    "                size=d_model, modal_num=self.modal_num, mm_atten=mm_attention, mt_atten=mt_attention,\n",
    "                feed_forward=feed_forward, dropout=0.5\n",
    "            ))\n",
    "        self.encoder = MultiModalEncoder(layer=multimodal_encoder_layer, N=8, modal_num=self.modal_num)\n",
    "        \n",
    "        self.regress = nn.Sequential(\n",
    "            nn.Linear(in_features=d_model * 4, out_features=d_model * 4 // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(d_model * 4 // 2),\n",
    "            nn.Linear(in_features=d_model * 4 // 2, out_features=7)\n",
    "        )\n",
    "        \n",
    "        self.final_activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, v_input: torch.Tensor=None, a_input: torch.Tensor=None, au_input: torch.Tensor=None):\n",
    "        batch_size, _, _ = v_input.shape\n",
    "        input_dict = {'visual': torch.concat((v_input, au_input[:, :, :-1]), dim=-1), 'acoustic': a_input}\n",
    "        \n",
    "        _x_list = []\n",
    "        for modality_type, input_tensor in input_dict.items():\n",
    "            _x_list.append(self.input[modality_type](input_tensor))\n",
    "            \n",
    "        x = torch.cat(_x_list, dim=-1)\n",
    "        x = self.dropout_embed(x)\n",
    "        \n",
    "        out = self.encoder(torch.cat(_x_list, dim=-1), mask=None)\n",
    "        outs = self.final_activation(self.regress(torch.cat((out, x), dim=-1)))\n",
    "        return outs.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "793c563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelsEnsemble(nn.Module): # 0.3975\n",
    "    def __init__(self, models_checkpoint: list = None, models_class: list = None, n_targets: int = 7):\n",
    "        super(ModelsEnsemble, self).__init__()\n",
    "        self.pretrained_models = nn.ModuleList()\n",
    "        for i in range(len(models_checkpoint)):\n",
    "            current_pretrained_model = models_class[i]\n",
    "            current_pretrained_model.load_state_dict(torch.load(models_checkpoint[i]))\n",
    "            current_pretrained_model.eval()\n",
    "            \n",
    "            self.pretrained_models.append(current_pretrained_model)\n",
    "            \n",
    "        self.ensemble_evaluator = nn.Sequential(\n",
    "            nn.Linear(in_features=n_targets * len(models_checkpoint), out_features=16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=16, out_features=n_targets)\n",
    "        )\n",
    "        self.final_activation = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, v_input: torch.Tensor=None, a_input: torch.Tensor=None, au_input: torch.Tensor=None):\n",
    "        total_predictions = []\n",
    "        for i in range(len(self.pretrained_models)):\n",
    "            current_pretrained_model = self.pretrained_models[i]\n",
    "            current_pretrained_model.eval()\n",
    "            prediction = current_pretrained_model(v_input, a_input, au_input)\n",
    "            total_predictions.append(prediction)\n",
    "        \n",
    "        total_predictions = torch.cat(total_predictions, dim=-1)\n",
    "        outs = self.final_activation(self.ensemble_evaluator(total_predictions))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4fecb852",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_checkpoint = [\n",
    "    f'{ABAW5_MODELS_CHECKPOINTS}/multiModal_0.3975.pt', f'{ABAW5_MODELS_CHECKPOINTS}/multiModal_0.3933.pt',\n",
    "    f'{ABAW5_MODELS_CHECKPOINTS}/multiModal_0.3906.pt'\n",
    "]\n",
    "models_class = [Model_3975().to(DEVICE), Model_3975().to(DEVICE), Model_3906().to(DEVICE)]\n",
    "\n",
    "model = ModelsEnsemble(models_checkpoint=models_checkpoint, models_class=models_class).to(DEVICE)\n",
    "regression_criterion, classification_criterion = nn.MSELoss(), nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903f5ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_3975().to(DEVICE)\n",
    "model.load_state_dict(torch.load(f'{ABAW5_MODELS_CHECKPOINTS}/multiModal_0.3975.pt'))\n",
    "model.eval()\n",
    "\n",
    "model33 = Model_3975().to(DEVICE)\n",
    "model33.load_state_dict(torch.load(f'{ABAW5_MODELS_CHECKPOINTS}/multiModal_0.3933.pt'))\n",
    "model33.eval()\n",
    "\n",
    "model06 = Model_3906().to(DEVICE)\n",
    "model06.load_state_dict(torch.load(f'{ABAW5_MODELS_CHECKPOINTS}/multiModal_0.3906.pt'))\n",
    "model06.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
