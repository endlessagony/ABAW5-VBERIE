{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207c8d80",
   "metadata": {},
   "source": [
    "# preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ff7475",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cd8b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment imports\n",
    "import sys, os, shutil, glob, random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# visualization imports\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('classic')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6dadf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math, timm, time\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch import block\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from   torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from   torch.utils.data.dataloader import default_collate\n",
    "from   torch.optim.optimizer import Optimizer\n",
    "from   torch.nn import TransformerEncoderLayer\n",
    "\n",
    "# Torchvision import block\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# einpos import block\n",
    "import einops\n",
    "from   einops import rearrange, repeat\n",
    "from   einops.layers.torch import Rearrange\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "from typing import Optional\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a805c198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current packages version:\n",
      "* torch: 2.2.1;\n",
      "* timm: 0.6.12;\n",
      "* torchvision: 0.17.1\n"
     ]
    }
   ],
   "source": [
    "print(f'current packages version:\\n* torch: {torch.__version__};' \\\n",
    "      + f'\\n* timm: {timm.__version__};\\n* torchvision: {torchvision.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9264a4a",
   "metadata": {},
   "source": [
    "## variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "762f8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR, LOCAL_DIR, FE_MODELS_DIR = './data', './', './models/feature_extractors'\n",
    "\n",
    "TRAIN_MP4_DIR, TRAIN_WAV_DIR = f'{DATA_DIR}/train/mp4', f'{DATA_DIR}/train/wav'\n",
    "VALID_MP4_DIR, VALID_WAV_DIR = f'{DATA_DIR}/val/mp4', f'{DATA_DIR}/val/wav'\n",
    "\n",
    "TRAIN_MP4_FEATURES_DIR, TRAIN_WAV_FEATURES_DIR = f'{DATA_DIR}/train/mp4_features', f'{DATA_DIR}/train/wav_features'\n",
    "VALID_MP4_FEATURES_DIR, VALID_WAV_FEATURES_DIR = f'{DATA_DIR}/val/mp4_features', f'{DATA_DIR}/val/wav_features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77760015",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABAW5_MODELS_CHECKPOINTS = './models/abaw_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d87e0378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current connected device is cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Current connected device is {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da65355",
   "metadata": {},
   "source": [
    "## classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c887b90",
   "metadata": {
    "code_folding": [
     0,
     14,
     24,
     39,
     73
    ]
   },
   "outputs": [],
   "source": [
    "def sinusoidal_embedding(n_channels: int=None, dim: int=None):\n",
    "    '''\n",
    "    Function define Positional Encoding.\n",
    "    \n",
    "            Parameters:\n",
    "                n_channels (int): Number of channels in the input;\n",
    "                dim (int): Dimension value.\n",
    "    '''\n",
    "    pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)] for p in range(n_channels)])\n",
    "    pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
    "    pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
    "    \n",
    "    return rearrange(pe, '... -> 1 ...')\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    \"\"\"Class for layer normalization in the current network\"\"\"\n",
    "    def __init__(self, dim: int=None, fn: nn.Sequential=None):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x: torch.Tensor=None, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-forward block for the Transformer class\"\"\"\n",
    "    def __init__(self, dim: int=None, hidden_dim: int=None, dropout: float=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor=None):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism realiztion\"\"\"\n",
    "    def __init__(self, dim: int=None, heads: int=8, dim_head: int=64, dropout: float=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor=None):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        \n",
    "        return self.to_out(out)\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer block class\"\"\"\n",
    "    def __init__(\n",
    "        self, dim: int=None, depth: int=None, heads: int=None, \n",
    "        dim_head: int=None, mlp_dim: int=None, dropout: float=0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "            \n",
    "    def forward(self, x: torch.Tensor=None):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e15bf7",
   "metadata": {
    "code_folding": [
     3,
     21
    ]
   },
   "outputs": [],
   "source": [
    "def get_prob(\n",
    "    features:np.ndarray=None, classifier_weights:np.ndarray=None, classifier_bias:np.ndarray=None, \n",
    "    logits:bool=True\n",
    ") -> np.ndarray:\n",
    "    '''\n",
    "    Function for the getting probabilities of the classes of the feature_extraction model.\n",
    "\n",
    "            Parameters:\n",
    "                features (np.ndarray): Current extracted features;\n",
    "                classifier_weights (np.ndarray): Classifier weights;\n",
    "                classifier_bias(np.ndarray): Classifier bias;\n",
    "                logits (bool): Get the logits or not.\n",
    "    '''\n",
    "    xs = np.dot(features, np.transpose(classifier_weights)) + classifier_bias\n",
    "\n",
    "    if logits:\n",
    "        return xs\n",
    "    else:\n",
    "        e_x = np.exp(xs - np.max(xs, axis=1)[:,np.newaxis])\n",
    "        return e_x / e_x.sum(axis=1)[:, None]\n",
    "\n",
    "def stack_npy_files(directory: str=None):\n",
    "    \"\"\"\n",
    "    Open all .npy files in the given directory and stack them into one big array.\n",
    "    \n",
    "            Parameters:\n",
    "                directory (str): The directory containing the .npy files.\n",
    "    \"\"\"\n",
    "    npy_files = [file for file in os.listdir(directory) if file.endswith('.npy')]\n",
    "    if not npy_files:\n",
    "        print(f\"No .npy files found in the directory {directory}.\")\n",
    "        return None\n",
    "    \n",
    "    stacked_array = np.concatenate([np.load(os.path.join(directory, file)) for file in npy_files], axis=0)\n",
    "    return stacked_array\n",
    "\n",
    "class AcousticModalityDataset(Dataset):\n",
    "    \"\"\"Dataset class for the visual-modality\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, data_info_path: str=None, split: str='Train', acoustic_features_dir_path: str=None,\n",
    "        use_mean: bool=False, max_seq_length: int=400\n",
    "    ):\n",
    "        super(AcousticModalityDataset, self).__init__()\n",
    "        self.data_info = pd.read_csv(data_info_path)\n",
    "        self.split, self.acoustic_features_dir_path = split, acoustic_features_dir_path\n",
    "        self.data_info = self.data_info[self.data_info['Split'] == self.split]\n",
    "        self.file_name_padding, self.target_labels = 5, [\n",
    "            'Adoration', 'Amusement', 'Anxiety', 'Disgust','Empathic-Pain', 'Fear', 'Surprise'\n",
    "        ]\n",
    "        \n",
    "        self.use_mean, self.max_seq_length = use_mean, max_seq_length\n",
    "        \n",
    "        self.__init_inputs_labels()\n",
    "        \n",
    "    def __init_inputs_labels(self):\n",
    "        print(f'Configure dataset from directory {self.acoustic_features_dir_path}')\n",
    "        self.inputs, self.meta, self.labels = [], [], []\n",
    "        \n",
    "        for index, row_values in tqdm(self.data_info.iterrows(), total=self.data_info.shape[0]):\n",
    "            current_ID = str(row_values['ID'])\n",
    "            current_file_ID = '0' * (self.file_name_padding - len(current_ID)) + current_ID\n",
    "            \n",
    "            feature_dir_path = f'{self.acoustic_features_dir_path}/{current_file_ID}_features'\n",
    "            if len(os.listdir(feature_dir_path)) < 1:\n",
    "                continue\n",
    "            current_acostic_features = np.load(f'{feature_dir_path}/feature-extraction-model_features.npy')\n",
    "            \n",
    "            seq_length = current_acostic_features.shape[0]\n",
    "            if seq_length > self.max_seq_length: \n",
    "                current_acostic_features = current_acostic_features[:self.max_seq_length]\n",
    "            else:\n",
    "                current_acostic_features = np.pad(\n",
    "                    current_acostic_features, pad_width=((0, self.max_seq_length - seq_length),(0,0)))\n",
    "            \n",
    "            if self.use_mean:\n",
    "                current_acostic_features = np.mean(current_acostic_features, axis=0)\n",
    "            current_labels = row_values[self.target_labels].values\n",
    "            current_meta_info = [row_values['Age'], int(row_values['Country'] in ['United States'])]\n",
    "            \n",
    "            self.inputs.append(current_acostic_features)\n",
    "            self.labels.append(current_labels)\n",
    "            self.meta.append(current_meta_info)\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index: int=None):\n",
    "        inputs, labels, meta_info = self.inputs[index], self.labels[index], self.meta[index]\n",
    "        \n",
    "        inputs = torch.tensor(inputs.astype(np.float32), dtype=torch.float)\n",
    "        labels = torch.tensor(labels.astype(np.float32), dtype=torch.float)\n",
    "        meta_info = torch.tensor(np.array(meta_info).astype(np.float32), dtype=torch.float)\n",
    "        \n",
    "        return inputs, labels, meta_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc907b2",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "106b9354",
   "metadata": {
    "code_folding": [
     0,
     17,
     38,
     49
    ]
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed:int=None) -> None:\n",
    "    '''\n",
    "    Function seed every random asprect.\n",
    "\n",
    "            Parameters:\n",
    "                seed (int): The seed number.\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def configure_feature_extraction_model_visual(\n",
    "    feature_extractor_model_path: str=None, device: torch.device=None, return_initial: bool=True\n",
    ") -> np.ndarray:\n",
    "    '''\n",
    "    Function configure feature-extraction model\n",
    "    \n",
    "            Parameters:\n",
    "                extraction_model_path (str): path to feature-extraction model;\n",
    "                device (torch.device): torch device (default=torch.cuda);\n",
    "                return_initial (bool): Return the initial model or not\n",
    "            Returns:\n",
    "                features-extraction model\n",
    "    '''\n",
    "    feature_extractor_model = torch.load(feature_extractor_model_path)\n",
    "    feature_extractor_model.classifier = torch.nn.Identity()\n",
    "    feature_extractor_model.to(device)\n",
    "    feature_extractor_model.eval()\n",
    "\n",
    "    if return_initial:\n",
    "        return feature_extractor_model, torch.load(feature_extractor_model_path)\n",
    "    else:\n",
    "        return feature_extractor_model\n",
    "    \n",
    "def calc_pearsons(predictions:np.array=None, ground_truth:np.array=None):\n",
    "    '''\n",
    "    Function calculates Pearson's Correlation Coefficient.\n",
    "    \n",
    "            Parameters:\n",
    "                predictions (np.array): Model's forecasts;\n",
    "                ground_truth (np.array): The fact.\n",
    "    '''\n",
    "    pcc = stats.pearsonr(predictions, ground_truth)\n",
    "    return pcc[0]\n",
    "\n",
    "def mean_pearsons(predictions:np.array=None, ground_truth:np.array=None, n_classes:int=7):\n",
    "    '''\n",
    "    Function calculates mean PCC between predictions and fact.\n",
    "    \n",
    "            Parameters:\n",
    "                predictions (np.array): Model's forecasts;\n",
    "                ground_truth (np.array): The fact;\n",
    "                n_classes (int): number of classes.\n",
    "    '''\n",
    "    predictions, ground_truth = predictions.detach().cpu().numpy(), ground_truth.detach().cpu().numpy()\n",
    "    class_wise_pcc = np.array([calc_pearsons(predictions[:, i], ground_truth[:, i]) for i in range(n_classes)])\n",
    "    mean_pcc = np.mean(class_wise_pcc)\n",
    "    \n",
    "    return mean_pcc, class_wise_pcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cab52fa",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch_visual(\n",
    "    model:nn.Module=None, train_dataloader:DataLoader=None, criterion:nn.Module=None, \n",
    "    optimizer:torch.optim.Optimizer=None, scheduler:torch.optim.lr_scheduler=None, device=None, mode:str=None,\n",
    "    use_meta: bool=False\n",
    "):\n",
    "    '''\n",
    "    Function perform one epoch train iteration.\n",
    "    \n",
    "            Parameters:\n",
    "                model (nn.Module): Current model;\n",
    "                train_dataloder (Dataloader): Current train dataloader;\n",
    "                criterion (nn.Module): Loss-function;\n",
    "                optimizer (torch.optim.Optimizer): Current optimization function;\n",
    "                scheduler (torch.optim.Scheduler): Current scheduler;\n",
    "                device: Current device;\n",
    "                mode (str): Current mode ('only-visual', 'only-audio', 'multi-modal')\n",
    "    '''\n",
    "    train_loss, train_mean_pcc, total_outputs, total_labels = 0, 0, [], []\n",
    "    model.train()\n",
    "    \n",
    "    for current_batch in tqdm(train_dataloader):\n",
    "        visual_features, labels, meta_info = current_batch\n",
    "        visual_features, labels, meta_info = torch.tensor(data=visual_features, dtype=torch.float32), \\\n",
    "            torch.tensor(data=labels, dtype=torch.float32), torch.tensor(data=meta_info, dtype=torch.float32)\n",
    "        visual_features, labels, meta_info = visual_features.to(device), labels.to(device), meta_info.to(device)\n",
    "        \n",
    "        if use_meta: visual_features = torch.concat((visual_features, meta_info.unsqueeze(1).expand(-1, 128, -1)), dim=-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(visual_features)\n",
    "        \n",
    "        train_batch_loss = criterion(labels, outputs)\n",
    "        train_batch_loss.backward()\n",
    "        train_loss += train_batch_loss.item()\n",
    "        \n",
    "        batch_mean_pcc, _ = mean_pearsons(outputs, labels)\n",
    "        train_mean_pcc += batch_mean_pcc\n",
    "        \n",
    "        optimizer.step()\n",
    "            \n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "    train_mean_pcc = train_mean_pcc / len(train_dataloader)\n",
    "    \n",
    "    return train_loss, train_mean_pcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fadea54b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def eval_one_epoch_visual(\n",
    "    model:nn.Module=None, valid_dataloader:DataLoader=None, criterion:nn.Module=None, \n",
    "    scheduler:torch.optim.lr_scheduler=None, device=None, mode:str=None,\n",
    "    use_meta: bool=False, model_checkpoint_dir: str=None, best_validation_score: float=None,\n",
    "    model_suffix: str=None\n",
    "):\n",
    "    '''\n",
    "    Function perform one epoch train iteration.\n",
    "    \n",
    "            Parameters:\n",
    "                model (nn.Module): Current model;\n",
    "                train_dataloder (Dataloader): Current train dataloader;\n",
    "                criterion (nn.Module): Loss-function;\n",
    "                optimizer (torch.optim.Optimizer): Current optimization function;\n",
    "                scheduler (torch.optim.Scheduler): Current scheduler;\n",
    "                device: Current device;\n",
    "                mode (str): Current mode ('only-visual', 'only-audio', 'multi-modal')\n",
    "    '''\n",
    "    valid_loss, valid_mean_pcc, total_outputs, total_labels = 0, 0, [], []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for current_batch in tqdm(valid_dataloader):\n",
    "            visual_features, labels, meta_info = current_batch\n",
    "            visual_features, labels, meta_info = torch.tensor(data=visual_features, dtype=torch.float32), \\\n",
    "                torch.tensor(data=labels, dtype=torch.float32), torch.tensor(data=meta_info, dtype=torch.float32)\n",
    "            visual_features, labels, meta_info = visual_features.to(device), labels.to(device), meta_info.to(device)\n",
    "\n",
    "            if use_meta: visual_features = torch.concat(\n",
    "                (visual_features, meta_info.unsqueeze(1).expand(-1, 128, -1)), dim=-1)\n",
    "\n",
    "            outputs = model(visual_features)\n",
    "\n",
    "            valid_batch_loss = criterion(labels, outputs)\n",
    "            valid_loss += valid_batch_loss.item()\n",
    "\n",
    "            batch_mean_pcc, _ = mean_pearsons(outputs, labels)\n",
    "            valid_mean_pcc += batch_mean_pcc\n",
    "\n",
    "        valid_loss = valid_loss / len(valid_dataloader)\n",
    "        valid_mean_pcc = valid_mean_pcc / len(valid_dataloader)\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    if valid_mean_pcc >= best_validation_score:\n",
    "        torch.save(model.state_dict(), f'{model_checkpoint_dir}/{model_suffix}_{valid_mean_pcc:.4f}.pt')\n",
    "        \n",
    "    return valid_loss, valid_mean_pcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbbd9ae",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe4a09",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2fa8c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7130b3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configure dataset from directory ./data/train/wav_features-feature-extraction-model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac55dd19a67444a9788cd41325c0a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15806 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configure dataset from directory ./data/val/wav_features-feature-extraction-model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f302cb7039674a54835b05d05458ec2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset were configured in 4.0 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time_extraction = time.time()\n",
    "\n",
    "train_dataset = AcousticModalityDataset(\n",
    "    data_info_path=f'{DATA_DIR}/preprocessed_data_info.csv', split='Train', \n",
    "    acoustic_features_dir_path=TRAIN_WAV_FEATURES_DIR + '-feature-extraction-model'\n",
    ")\n",
    "valid_dataset = AcousticModalityDataset(\n",
    "    data_info_path=f'{DATA_DIR}/preprocessed_data_info.csv', split='Val', \n",
    "    acoustic_features_dir_path=VALID_WAV_FEATURES_DIR + '-feature-extraction-model'\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time_extraction\n",
    "print(f'Dataset were configured in {np.round((elapsed_time/60), 0)} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096bd6d1",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2efb6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator(object):\n",
    "    \"\"\"Class for appropriate collate_fn\"\"\"\n",
    "    def __init__(self, imgRandomLen: int=64):\n",
    "        super().__init__()\n",
    "        self.imgRandomLen = imgRandomLen\n",
    "\n",
    "    def __call__(self, batch: torch.Tensor=None):\n",
    "        '''\n",
    "        Select a specfic number of images randomly for the time being.\n",
    "        \n",
    "                Parameters:\n",
    "        '''\n",
    "        audio = np.stack([tensors[0] for tensors in batch], axis=0).astype(np.float)\n",
    "        labels = np.stack([tensors[1] for tensors in batch], axis=0).astype(np.float)\n",
    "        meta   = np.stack([tensors[2] for tensors in batch], axis=0).astype(np.float)\n",
    "        \n",
    "        return np.stack(audio, axis=0).astype(np.float), labels, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92c38f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = Collator()\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbf32ce",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "125f6626",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcousticModalityModel(nn.Module):\n",
    "    def __init__(self, input_size:int=None, n_classes:int=7):\n",
    "        super(AcousticModalityModel, self).__init__()\n",
    "        self.rnn_aud = nn.LSTM(768, 128, 2, batch_first=True)\n",
    "        self.transformer_aud = Transformer(128, 4, 4, dim_head=128, mlp_dim=256, dropout=0.2)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, 7),\n",
    "        )\n",
    "    \n",
    "    def forward(self, acoustic_input: torch.Tensor=None):\n",
    "        acoustic_input, hidden_state = self.rnn_aud(acoustic_input)\n",
    "        acoustic_input = self.transformer_aud(self.dropout(acoustic_input))\n",
    "        \n",
    "        return torch.sigmoid(self.head(acoustic_input)).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ad931695",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AcousticModalityModel(input_size=768).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae2fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs, model_suffix = 150, 'acoustic'\n",
    "all_checkpoints = [\n",
    "    float(checkpoint.split('_')[-1].replace('.pt', '')) for checkpoint \n",
    "    in os.listdir(ABAW5_MODELS_CHECKPOINTS) if ((checkpoint.endswith('.pt')) & (model_suffix in checkpoint))\n",
    "]\n",
    "best_val_pcc = max(all_checkpoints)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    if epoch in range(9):\n",
    "        print(f\"Epoch 0{epoch+1}/{n_epochs}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "        \n",
    "    train_loss, train_mean_pcc = train_one_epoch_visual(\n",
    "        model=model, train_dataloader=train_dataloader, criterion=criterion, optimizer=optimizer, \n",
    "        device=DEVICE, use_meta=False\n",
    "    )\n",
    "    print(f'training results. mean_pcc: {train_mean_pcc:.4f}; loss: {train_loss:.4f}.')\n",
    "    \n",
    "    valid_loss, valid_mean_pcc = eval_one_epoch_visual(\n",
    "        model=model, valid_dataloader=valid_dataloader, device=DEVICE, use_meta=False, criterion=criterion,\n",
    "        model_checkpoint_dir=ABAW5_MODELS_CHECKPOINTS, best_validation_score=best_val_pcc,\n",
    "        model_suffix=model_suffix\n",
    "    )\n",
    "    print(f'validating results. mean_pcc: {valid_mean_pcc:.4f}; loss: {valid_loss:.4f}. best result: {best_val_pcc:.4f}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
